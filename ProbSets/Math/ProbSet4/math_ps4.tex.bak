\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{frame=single,
  language=Python,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{mathrsfs}

\usepackage{harvard}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}

\hypersetup{colorlinks,linkcolor=red,urlcolor=blue}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
%\numberwithin{equation}{section}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}

\usepackage{mathtools}
% commands
\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepackage{mathrsfs}


\begin{document}

\begin{flushleft}
  \textbf{\large{Problem Set \#4, Optimization}} \\
  OSM Lab: Math \\
  Rebekah Dix
\end{flushleft}

\vspace{5mm}

\subsubsection*{Exercise 6.6}
Suppose
\begin{equation}
	f(x,y) = 3x^2 y + 4xy^2 + xy 
\end{equation}
Then, 
\begin{align*}
	\frac{\partial f}{\partial x} &= 6xy + 4y^2 + y = y(6x + 4y + 1)\\
	\frac{\partial f}{\partial y} &= 3x^2 + 8xy + x = x(3x + 8y + 1)
\end{align*}
We now find the critical points of $f$. Observe that $(0,0)$ is a critical point. Similarly, if we set $x = 0$, then $(0, -\frac{1}{4})$ is a critical point. If we set $y = 0$, then $(-\frac{1}{3}, 0)$ is a critical point. Finally, observe that the values of $x$ and $y$ satisfying $6x + 4y = -1$ and $3x + 8y = -1$ will also be a critical point. This occurs at $(-\frac{1}{9}, -\frac{1}{12})$. Now, the Hessian is given by,
\begin{equation}
	D^2 f(x,y) = 
	\begin{bmatrix}
	6y & 6x + 8y  + 1\\
	6x + 8y + 1 & 8x
	\end{bmatrix}
\end{equation}
Then, 
\begin{align*}
	D^2f(0,0) &= 
	\begin{bmatrix}
	0 & 1 \\
	1 & 0 
	\end{bmatrix} \qquad
	D^2f(0,-\frac{1}{4}) = 
	\begin{bmatrix}
	-\frac{3}{2} & -1 \\
	-1 & 0 
	\end{bmatrix} \\
	D^2f(-\frac{1}{3},0) &= 
	\begin{bmatrix}
	0 & -1 \\
	-1 & -\frac{8}{3} 
	\end{bmatrix} \qquad
	D^2f(0,-\frac{1}{4}) = 
	\begin{bmatrix}
	-\frac{1}{2} & -\frac{1}{3} \\
	-\frac{1}{3} & -\frac{8}{9} 
	\end{bmatrix} 
\end{align*}
$D^2 f(0, -\frac{1}{4})$ is negative semidefinite, so $(0, -\frac{1}{4})$ is a local maximum. However, the remaining Hessian matrices are neither positive semidefinite nor negative semidefinite. Therefore, those corresponding points are saddle points. 
  
\subsubsection*{Exercise 6.7} [[Incomplete]]
Define an unconstrained quadratic optimization problem as follows: the objective function $f: \mathbb{R}^n \to \mathbb{R}$ is quadratic,
\begin{equation}\label{quad1}
	f(x) = x^TAx - b^Tx + c
\end{equation}
where $A \in M_n(\mathbb{R})$, $b\in\mathbb{R}^n$, and $c \in \mathbb{R}$.
\begin{claim}
	For any square matrix $A \in M_n(\mathbb{R})$, the matrix $Q = A^T + A$ is symmetric, and $x^TQx = x^TA^Tx + x^TAx = 2x^TAx$, so Equation \ref{quad1} is equal to,
	\begin{equation}\label{quad2}
	f(x) = \frac{1}{2} x^TQx - b^Tx + c
	\end{equation}
\end{claim}
\begin{proof}
	Observe that,
\begin{equation}
	Q = A^T + A = A + A^T = (A^T + A)^T = Q^T
\end{equation}
	Therefore $Q$ is symmetric. Additionally,
	\begin{equation}
	x^TQx = x^T(A^T + A)x = x^TA^Tx + x^TAx = 2x^TAx
	\end{equation}
	Where the first equality follows from the definition of $Q$, the second from expanding the sum, and the third because $x^TA^Tx$ and $x^TAx$ are scalars. The transpose of a scalar is simply that scalar, so $(x^TA^Tx)^T = x^TAx$. Thus, $x^TA^Tx + x^TAx = 2x^TAx$. It follows that $x^TAx = \frac{1}{2} x^TQx$. Therefore, we may substitute this expression into Equation \ref{quad1} to obtain Equation \ref{quad2}.
\end{proof}

\begin{claim}
	Any minimizer $x^*$ of $f$ is a solution of the equation,
	\begin{equation}
		Q^T x^* = b
	\end{equation}
\end{claim}
\begin{proof}
	First note that $f$ is differentiable at $x^*$ and $x^*$ is an interior point. Therefore, the first order necessary condition requires that $f'(x^*) = 0$ if $x^*$ is a local minimizer. Note that $f'(x) = Q^T x - b$. Therefore, if $x^*$ is a local minimizer of $f$, then $Q^T x^* = b$. 
\end{proof}

\begin{claim}
	The quadratic minimization problem will have a unique solution if and only if $Q$ is positive definite. 
\end{claim}
\begin{proof}
	First suppose that $Q$ is positive definite. Thus, it follows that $f''(x) >0$ for all $x$. Then, $Q$ is invertible, so that $x^* = Q^{-1}b$ for a local minimizer $x^*$. Thus, by the second-order sufficient condition, $x^*$ is a unique minimizer of (\ref{quad2}). Conversely, suppose that $x^*$ is the unique minimizer of $f$. Then, by the second order necessary condition, we have that $Q$ is positive semidefinite. 
\end{proof}

\subsubsection*{Exercise 6.11}
Let $f(x) = ax^2 + bx + c$ be a quadratic function with $a > 0$. Let $x_0 \in \mathbb{R}$. Then, applying Newton's method, observe that,
\begin{equation}
	x_1 = x_0 - \frac{f'(x_0)}{f''(x_0)} = x_0 - \frac{2ax_0 + b}{2a} = - \frac{b}{2a}
\end{equation}
Observe that $f'(x_1) = 2a x_1 + b = 2a ( - \frac{b}{2a}) + b = 0$. Then, $f''(x_1) = 2a >0$ since $ a> 0$. Therefore, by the second-order sufficient condition, $x_1 = -\frac{b}{2a}$ is a unique local minimizer.  

\subsubsection*{Exercise 6.15}
See Jupyter notebook.

\subsubsection*{Exercise 7.1}\
\begin{claim}
	If $S$ is a nonempty subset of $V$, then $conv(S)$ is convex.
\end{claim}
\begin{proof}
	Let $y_1, y_2 \in conv(S)$. Therefore, there exist $k \in \mathbb{N}$ and $(\lambda_i)_{i=1}^k$ such that $y = \sum_{i=1}^k \lambda_i y_i$, where each $\lambda_i \geq 0$, $\sum_{i=1}^{k} \lambda_i = 1$, and each $y_i \in S$. Similarly, there exist $l \in \mathbb{N}$ and $(\mu_i)_{i=1}^l$ such that $x = \sum_{i=1}^l \mu_i x_i$, where each $\mu_i \geq 0$, $\sum_{i=1}^{l} \mu_i = 1$, and each $x_i \in S$. Fix $\lambda \in [0,1]$. Then,
	\begin{equation}
	\lambda x+ (1 - \lambda)y = \lambda\sum_{i=1}^l \mu_i x_i + (1 - \lambda) \sum_{i=1}^k \lambda_i y_i
	\end{equation}
	Consider the coefficients on the $x_i$'s and $y_i$'s. Note that,
	\begin{equation}
	\lambda( \sum_{i=1}^l \mu_i ) + (1 - \lambda) \sum_{i=1}^k \lambda_i = \lambda + (1 - \lambda) = 1
	\end{equation}
	Observe that $\lambda x+ (1 - \lambda)y$ is indeed another convex combination of the elements of $S$. Therefore, $\lambda x+ (1 - \lambda)y \in conv(S)$ so $conv(S)$ is convex.
\end{proof}

\subsubsection*{Exercise 7.2}
\begin{claim}
	A hyperplane is convex.
\end{claim}
\begin{proof}
	Let $(V, \inp{\cdot}{\cdot})$ be an inner product space. Let $a \in V$ where $a \neq 0$ and $b \in \mathbb{R}$. Let $P = \{x \in V : \inp{a}{x} = b\}$ be hyperplane in $V$. Let $x, y \in P$ and $\lambda \in [0,1]$. By the definition of $P$, we have that $\inp{a}{x} = b$ and $\inp{a}{y} = b$. Therefore, by the linearity of the inner product, we have that,
	\begin{align*}
	b &= \lambda b + (1 - \lambda)b \\
	&= \lambda \inp{a}{x} + (1 - \lambda) \inp{a}{y} \\
	&= \inp{a}{\lambda x} +  \inp{a}{(1-\lambda)y} \\
	&= \inp{a}{\lambda x + (1-\lambda)y}
	\end{align*}
	Therefore, $\inp{a}{\lambda x + (1-\lambda)y} = b$, therefore $\lambda x + (1-\lambda)y \in P$, so that $P$ is a convex set by definition.
\end{proof}
\begin{claim}
	A half space is convex.
\end{claim}
\begin{proof}
	Let $(V, \inp{\cdot}{\cdot})$ be an inner product space. Let $a \in V$ where $a \neq 0$ and $b \in \mathbb{R}$. Let $H = \{x \in V : \inp{a}{x} \leq b\}$ be hyperplane in $V$. Let $x, y \in H$ and $\lambda \in [0,1]$. By the definition of $H$, we have that $\inp{a}{x} \leq b$ and $\inp{a}{y} \leq b$. Therefore, by the linearity of the inner product, we have that,
	\begin{align*}
	b &= \lambda b +  (1 - \lambda) b \\
	&\geq \lambda \inp{a}{x} + (1 - \lambda) \inp{a}{y} \\
	&= \inp{a}{\lambda x + (1-\lambda)y} 
	\end{align*}
	Therefore, $\inp{a}{\lambda x + (1-\lambda)y} \leq b$, so that $\lambda x + (1-\lambda)y \in H$. Thus, $H$ is convex.
\end{proof}

\subsubsection*{Exercise 7.4} 
\begin{claim}\label{clm1}
	Let $x, y, p \in \mathbb{R}^n$. Then,
	\begin{equation}
		\norm{x-y}^2 = \norm{x-p}^2 + \norm{p-y}^2 + 2\inp{x-p}{p-y}
	\end{equation}
\end{claim}
\begin{proof}
Observe that,
\begin{align*}
	\norm{x-y}^2 &= \norm{x -p + p -y}^2 \\
	&= \inp{x -p + p -y}{x -p + p -y} \\
	&= \inp{x-p}{x-p} + \inp{x-p}{p-y} + \inp{p-y}{x-p} + \inp{p-y}{p-y} \\
	&= \norm{x-p}^2 + \norm{p-y}^2 + 2\inp{x-p}{p-y}
\end{align*}
\end{proof}

\begin{claim}\label{iff}
	Let $C \in \mathbb{R}^n$ be nonempty, closed, and convex. Let $p \in C$. Suppose that $\inp{x-p}{p-y} \geq 0$ for all $y \in C$. Then, $\norm{x - y} > \norm{x-p}$ for all $y \in C$, $y \neq p$. 
\end{claim}
\begin{proof}
	By Claim \ref{clm1}, we know that $\norm{x-y}^2 = \norm{x-p}^2 + \norm{p-y}^2 + 2\inp{x-p}{p-y}$. By the definition of a norm, and by squaring each norm, we know that $\norm{x-y}^2, \norm{x-p}^2, \norm{p-y}^2$ are all weakly positive. Furthermore, by assumption, $\inp{x-p}{p-y}$ is weakly positive. Then, since $y \neq p$, we know that $ p - y \neq 0$ so that $\norm{p-y}^2 > 0$. Therefore, 
	\begin{align*}
		\norm{x-y}^2 &= \norm{x-p}^2 + \norm{p-y}^2 + 2\inp{x-p}{p-y} \\
		&\geq \norm{x-p}^2 + \norm{p-y}^2 \\
		&>\norm{x-p}^2
	\end{align*}
	Therefore, $\norm{x-y} > \norm{x-p}$. 
\end{proof}

\begin{claim}\label{clm3}
	If $z = \lambda y + (1 - \lambda)p$, where $0 \leq \lambda \leq 1$, then,
	\begin{equation}
		\norm{x-z}^2 = \norm{x-p}^2 + 2\lambda\inp{x-p}{p-y} + \lambda^2 \norm{y-p}^2
	\end{equation}
\end{claim}
\begin{proof}
	By the linearity of the inner product, we have that,
	\begin{align*}
	\norm{x-z}^2 &= \norm{x-\lambda y - (1 - \lambda)p}^2 \\
	&= \norm{x - p +\lambda(p - y)}^2 \\
	&= \inp{x-p}{x-p} + 2\lambda\inp{x-p}{p-y} + \lambda^2\norm{p-y}^2 \\
	&=  \norm{x-p}^2+ 2\lambda\inp{x-p}{p-y} + \lambda^2\norm{y-p}^2
	\end{align*}
\end{proof}

\begin{claim} \label{if}
	If $p$ is a projection of $x$ onto the convex set $C$, then $\inp{x-p}{p-y} \geq 0$ for all $y \in C$. 
\end{claim}
\begin{proof}
	By Claim \ref{clm3}, we know that, $\norm{x-z}^2 = \norm{x-p}^2 + 2\lambda\inp{x-p}{p-y} + \lambda^2 \norm{y-p}^2$. Observe that $\norm{x-z}^2, \norm{x-p}^2, \lambda$ are all weakly positive. Therefore, 
	\begin{align*}
	0 &\leq \norm{x-z}^2 = \norm{x-p}^2 + 2\lambda\inp{x-p}{p-y} + \lambda^2 \norm{y-p}^2\\
	&\leq 2\lambda\inp{x-p}{p-y} + \lambda^2 \norm{y-p}^2
	\end{align*}
	Suppose $\lambda \in (0,1]$. It follows that $0 \leq 2\inp{x-p}{p-y} + \lambda \norm{y-p}^2$.
\end{proof}

\begin{claim}
	A point $p \in C$ is the projection of $x$ onto $C$ if and only if
	\begin{equation}
	\inp{x-p}{p-y} \geq 0 
	\end{equation}
	for all $y \in C$. 
\end{claim}
\begin{proof}
	First suppose $p \in C$ is the projection of $x$ onto $C$. In Claim \ref{if}, set $\lambda = 0$. It follows that $\inp{x-p}{p-y} \geq 0$ for all $y \in C$. Conversely, suppose $\inp{x-p}{p-y} \geq 0 $ for all $y \in C$. Then, by claim \ref{iff}, we have that $\norm{x - y} > \norm{x-p}$ for all $y \in C$, $y \neq p$. Therefore, $p \in C$ is the projection of $x$ onto $C$.
\end{proof}

\subsubsection*{Exercise 7.8}
\begin{claim}
	If $f: \mathbb{R}^m \to \mathbb{R}$ is convex, if $A \in M_{m \times n} (\mathbb{R})$, and if $b \in \mathbb{R}^m$, then the function $g: \mathbb{R}^m \to \mathbb{R}$ given by $g(x) = f(Ax + b)$ is convex.
\end{claim}
\begin{proof}
	Let $x_1, x_2 \in \mathbb{R}^m$ and let $\lambda \in [0,1]$. Then,
	\begin{align*}
	g(\lambda x_1 + (1 - \lambda)x_2) &= f(A (\lambda x_1 + (1 - \lambda)x_2) + b) \\
	&= f(\lambda(A x_1 + b) + (1-\lambda)(A x_2 + b)) \\
	&\leq \lambda f(A x_1 + b) + (1-\lambda) f(A x_2 + b) \tag{by the convexity of $f$}\\ 
	&= \lambda g(x_1) + (1 - \lambda) g(x_2) 
	\end{align*}
	Therefore, $g$ is convex.
\end{proof}

\subsubsection*{Exercise 7.12}
\begin{claim}
	The set $PD_n (\mathbb{R})$ of positive-definite matrices in $M_n (\mathbb{R})$ is convex.
\end{claim}
\begin{proof}
	Let $M$ and $N$ be positive-definite matrices. Fix $\lambda \in [0,1]$ and $x \in \mathbb{R}^n$. Then, by the definition of positive-definite and $\lambda$ being positive, we have that $x^T M x > 0$ and $x^TNx > 0$.  Consider the convex combination $\lambda M + (1-\lambda)N$. Then,
	\begin{equation} \label{matpos}
	x^T(\lambda M + (1-\lambda)N)x = \lambda x^TMx + (1 - \lambda)x^TNx
	\end{equation}
	If $\lambda \in (0,1)$, then Equation \ref{matpos} is clearly strictly positive. Similarly, in the boundary cases ($\lambda = 0$ or $\lambda = 1$), we also have that one of the terms in Equation \ref{matpos} will be strictly positive and the other 0, so that the entire term is strictly positive. Therefore, $\lambda x^TMx + (1 - \lambda)x^TNx > 0$ so that $\lambda M + (1-\lambda)N$ is positive-definite, hence  $PD_n (\mathbb{R})$ is convex.
\end{proof}

\begin{claim}
	The function $f(X) = -\log(\det(X))$ is convex on $PD_n (\mathbb{R})$.
\end{claim}
\begin{proof}
We first prove a series of lemmas. 
\begin{lemma}
	If for every $A,B \in PD_n (\mathbb{R})$ the function $g(t): [0,1] \to \mathbb{R}$ given by $g(t) =f(tA + (1-t)B)$ is convex, then $f$ is convex.
\end{lemma}
\begin{proof}
	Observe that,
	\begin{align*}
	f(tA + (1-t)B) &= g(t) \\
	&= g(1 \cdot t + (1 - t) \cdot 0) \\
	&\leq t g(1) + (1-t) g(0) \\
	&= t f(A) + (1 -t) f(B) 
	\end{align*}
	Therefore, $f$ is convex.
\end{proof}

\begin{lemma}
	If $A$ is positive definite then there is an $S$ such that $S^H S = A$. Furthermore, 
	\begin{align*}
	g(t) &= -\log(\det(S^H(tI + (1-t)(S^H)^{-1}BS^{-1})S)) \\
	&= -\log(\det(A)) -\log(\det(tI + (1-t)(S^H)^{-1}BS^{-1}))
	\end{align*}
\end{lemma}
	\begin{proof}
	By Proposition 4.5.7 in Volume I, we know that if $A$ is positive definite, then there exists a nonsingular matrix $S$ such that $A = S^H S$. Now, recall that for matrices $A$ and $B$, we have that $det(AB) = det(A)det(B)$. We can use this property and properties of logarithms to prove the rest of this lemma. To that end,
	\begin{align*}
	-\log(\det(S^H(tI + (1-t)(S^H)^{-1}BS^{-1})S)) &= -\log(\det(S^H)\det(tI + (1-t)(S^H)^{-1}BS^{-1}))\det(S)) \\
	&= -\log(\det(S^H)\det(S)\det(tI + (1-t)(S^H)^{-1}BS^{-1})) \\
	&= -\log(\det(S^HS)\det(tI + (1-t)(S^H)^{-1}BS^{-1})) \\
	&= -\log(\det(S^HS)) - log(\det(tI + (1-t)(S^H)^{-1}BS^{-1})) \\
	&= -\log(\det(A)) -\log(\det(tI + (1-t)(S^H)^{-1}BS^{-1}))
	\end{align*}
	\end{proof}
	
	\begin{lemma}
	\begin{equation}
	g(t) = - \sum_{i=1}^n \log(t + (1-t)\lambda_i) + -\log(\det(A))
	\end{equation}
	where $\lambda_1, \ldots, \lambda_n$ are the eigenvalues of $(S^H)^{-1}BS^{-1}$. 
	\end{lemma}
	\begin{proof}
	First note that the eigenvalues of $tI + (1-t)(S^H)^{-1}BS^{-1})$ are $\{t + (1-t)\lambda_i\}_{i=1}^n$. Indeed, suppose $\lambda_i$ and $x_i$ are an eigenvalue and eigenvector respectively of $(S^H)^{-1}BS^{-1}$. Then, 
	\begin{align*}
	(tI + (1-t)(S^H)^{-1}BS^{-1}))x_i &= tx_i + (1-t)\lambda_i x_i \\
	&= (t + (1-t)\lambda_i) x_i
	\end{align*}
	so that $(t + (1-t)\lambda_i)$ is an eigenvalue of $tI + (1-t)(S^H)^{-1}BS^{-1})$.  Next, recall that the determinant of a matrix is the product of its eigenvalues. Therefore,
	\begin{align*}
	g(t) &= -\log(\det(A)) -\log(\det(tI + (1-t)(S^H)^{-1}BS^{-1})) \\
	&= - \sum_{i=1}^n \log(t + (1-t)\lambda_i) + -\log(\det(A))
	\end{align*}
	\end{proof}
	
	\begin{lemma}
	$g''(t) \geq 0$ for all $t \in [0,1]$. 
	\end{lemma}
	\begin{proof}
	By direct calculation from the above lemma,
	\begin{equation}
	g'(t) = - \sum_{i=1}^n \frac{(1-\lambda_i)}{\log(t + (1-t)\lambda_i)}
	\end{equation}
	and then,
	\begin{equation}
	g''(t) = \sum_{i=1}^n \frac{(1-\lambda_i)^2}{\log(t + (1-t)\lambda_i)^2}
	\end{equation}
	Observe that $g''(t)$ is composed of a sum of squares and is therefore weakly positive. 
	\end{proof}
	Together these lemmas show that $f(X) = -\log(\det(X))$ is convex on $PD_n (\mathbb{R})$. 
\end{proof}

\subsubsection*{Exercise 7.13} 
\begin{claim}
	If $f: \mathbb{R}^n \to \mathbb{R}$ is convex and bounded above, then $f$ is constant. 
\end{claim}
\begin{proof}
	Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is convex and bounded above but $f$ is not constant. Therefore, there exist points $x_1, x_2 \in \mathbb{R}^n$ such that $f(x_1) \neq f(x_2)$. Fix $\lambda \in [0,1]$.  Finally, suppose $f(x) \leq M$ for all $x$. Now, observe that,
	\begin{align*}
	f(x_1) &= f\left(\lambda \frac{x_1 - (1 - \lambda)x_2}{\lambda} + (1 - \lambda)x_2\right) \\
	&\leq \lambda  f\left(\frac{x_1 - (1 - \lambda)x_2}{\lambda}\right)+ (1 - \lambda)f(x_2) \tag{by the convexity of $f$}
	\end{align*}
	Next, observe that by this inequality, we have that,
	\begin{equation}
	\frac{f(x_1) - (1 - \lambda) f(x_2)}{\lambda} \leq f\left(\frac{x_1 - (1 - \lambda)x_2}{\lambda}\right) \leq M
	\end{equation}
	by the boundedness of $f$ from above. Now observe that,
	\begin{equation}
	\frac{f(x_1) - f(x_2)}{\lambda} \leq \frac{f(x_1) - (1 - \lambda) f(x_2)}{\lambda}
	\end{equation}
	Therefore, we have that,
	\begin{equation}
	\frac{f(x_1) - f(x_2)}{\lambda} \leq  M
	\end{equation}
	However, as let $\lambda$ go to $0$, the term on the LHS grows without bound, which contradicts the derived inequality. Therefore, we have reached a contradiction, and it must be that  $f$ is constant. 
\end{proof}

\subsubsection*{Exercise 7.20}
\begin{claim}
 If $f: \mathbb{R}^n \to \mathbb{R}$ is convex and $-f$ is also convex, then $f$ is affine.
\end{claim}
\begin{proof}
	First note that if $f$ is affine, then there exist a linear transformation $L$ and $c \in \mathbb{R}$ such that $f(x) = L(x) + c$. An implication of $f$ being affine is that for $\lambda \in [0,1]$ and $x_1, x_2 \in \mathbb{R}^n$, we have that 
	\begin{align*}
	f(\lambda x_1 + (1 - \lambda)x_2) &= L(\lambda x_1 + (1 - \lambda)x_2) + c \\
	&= \lambda Lx_1 + (1 - \lambda)L x_2  + c \tag{since $L$ is a linear transformation} \\
	&= \lambda (f(x_1) - c) + (1 - \lambda) (f(x_2) - c) + c  \tag{since $f$ is affine} \\
	&= \lambda f(x_1) + (1 - \lambda) f(x_2)
	\end{align*}
	
	Now, for the sake of contradiction, suppose $f$ and $-f$ are convex but $f$ is not affine. Then, since $f$ is convex, for all $x_1, x_2 \in \mathbb{R}^n$ and $\lambda \in [0,1]$,
	\begin{equation}\label{leq}
	f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2) 
	\end{equation}
	Similarly, since $-f$ is convex, for all $x_1, x_2 \in \mathbb{R}^n$ and $\lambda \in [0,1]$,
	\begin{equation}\label{geq}
	-f(\lambda x_1 + (1 - \lambda) x_2) \leq -\lambda f(x_1) -(1 - \lambda) f(x_2) 
	\end{equation}
	which implies $f(\lambda x_1 + (1 - \lambda) x_2) \geq \lambda f(x_1) + (1 - \lambda) f(x_2)$. Therefore, putting (\ref{leq}) and (\ref{geq}) together, we have that $f(\lambda x_1 + (1 - \lambda) x_2) = \lambda f(x_1) + (1 - \lambda) f(x_2)$. However, this equation contradicts the implication of being affine shown above. Therefore, $f$ must be affine.
\end{proof}

\subsubsection*{Exercise 7.21}
\begin{proof}
	First suppose that $x^*$ is a local minimizer of $f(x)$. Therefore, there exists a neighborhood $\Omega$ such that $f(x^*) \leq f(x)$ for all $x \in \Omega$. Now, since $\phi$ is strictly increasing, applying $\phi$ will preserve inequalities, so we have that $\phi(f(x^*)) \leq \phi(f(x))$ for all $x \in \Omega$. Therefore, $x^*$ is a local minimizer of $\phi(f(x))$. Next, suppose $x^*$ is a local minimizer of $\phi(f(x))$. Therefore, there exists a neighborhood $\Omega'$ such that $\phi(f(x^*)) \leq \phi(f(x))$ for all $x \in \Omega'$. Now, since $\phi$ is strictly increasing, it is one-to-one, and hence has an inverse. Similarly, $\phi^{-1}$ is also strictly increasing. Therefore, we may apply $\phi^{-1}$ to both sides of inequalities and have them preserved. If follows that $f(x^*) \leq f(x)$ for all $x \in \Omega'$, so that $x^*$ is a local minimizer of $f$.
\end{proof}
\end{document}