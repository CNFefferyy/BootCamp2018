\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{frame=single,
  language=Python,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{mathrsfs}

\usepackage{harvard}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}

\hypersetup{colorlinks,linkcolor=red,urlcolor=blue}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
%\numberwithin{equation}{section}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}

\usepackage{mathtools}
% commands
\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepackage{mathrsfs}

\setcounter{MaxMatrixCols}{20}
\graphicspath{{.}}
\begin{document}

\begin{flushleft}
  \textbf{\large{Problem Set \#6, Algorithms for Unconstrained Nonlinear Optimization}} \\
  OSM Lab: Math \\
  Rebekah Dix
\end{flushleft}

\vspace{5mm}
\subsection*{Exercise 1}
\begin{claim}
An unconstrained linear objective function is either constant or has no minimum.
\end{claim}
\begin{proof}
Let $L$ be a linear objective function, and suppose that $L$ is not constant.  For the sake of contradiction, suppose $x^*$ is a minimizer of $L$.  Since $L$ is not constant, there exists an $x$ such that $Lx \neq Lx^*$. There are two cases to consider. If $Lx < Lx^*$, then we have a contradiction as $x^*$ is not a minimizer. Similarly, if $Lx > Lx^*$, then $L(x - x^*) > 0$. Observe that,
\begin{equation}
L(x^*  + x^* - x) = Lx^* - L(x - x^*) < Lx^*
\end{equation}
Therefore $x^*$ is not a minimizer. Thus, if $L$ is not constant, then it has no minimum. Conversely, if $L$ is constant, then it has a minimum, which is the constant value of the function.
\end{proof}

\subsection*{Exercise 2}
\begin{claim}
Let $b \in \mathbb{R}^m$ and $A \in M_{m \times n}(\mathbb{R})$. Then the problem of finding an $x^* \in \mathbb{R}^n$ to minimize $\norm{Ax-b}_2$ is equivalent to minimizing
\begin{equation}\label{quadmin}
x^TA^TAx - 2b^TAx
\end{equation}
Furthermore, minimizing Equation \ref{quadmin} is equivalent to solving the normal equation
\begin{equation}
A^TAx = A^Tb
\end{equation} 
\end{claim}
\begin{proof}
First observe that,
\begin{align*}
\norm{Ax-b}^2_2 &= \inp{Ax-b}{Ax-b} \\
&= (Ax - b)^T(Ax-b) \\
&= x^TA^TAx - x^TAb - b^TAx + b^Tb \\
&=  x^TA^TAx  - 2 b^TAx + b^Tb \tag{because $ x^TAb$ and $b^TAx$ are scalar}
\end{align*}
Next observe that $b^Tb$ is a constant. Therefore, minimizing $\norm{Ax-b}_2$ is equivalent to minimizing $x^TA^TAx  - 2 b^TAx$.   

All $x$ are interior, so the FONC requires that at a minimum, $2x^TA^TA - 2b^TA = 0$, or that, after taking the transpose of this condition, $A^TAx = A^Tb$. Additionally, since $A^TA$ is positive semidefinite, the second order condition will be satisfied. Therefore, minimizing Equation \ref{quadmin} is equivalent to solving the normal equation. 
\end{proof}

\subsection*{Exercise 3}
\subsubsection*{Gradient Descent}
\begin{itemize}
\item \textbf{Basic idea:} The negative of the gradient of a function is the direction of greatest decrease. Therefore, to minimize a function, we choose an initial point and continually move in the direction of greatest decrease.  
\item \textbf{Optimization problems it can and cannot solve:} This method requires the objective function to be differentiable. 
\item \textbf{Relative strengths:} This method converges quickly for problems with a low condition number.
\item \textbf{Relative weaknesses:} This method converges slowly for problems with a large condition number.
\end{itemize}

\subsubsection*{Newton and Quasi-Newton Methods}
\begin{itemize}
\item \textbf{Basic idea:} Newton's method creates a local quadratic approximation to the function using the gradient and Hessian at a specific point, and then iterates forward to the minimum of that quadratic approximation. 
\item \textbf{Optimization problems it can and cannot solve:} Newton's method is particularly powerful for quadratic optimization problems. 
\item \textbf{Relative strengths:} Newton's method converges quickly for problems where the dimension is not too large and the Hessian is positive definite and can be computed easily. If the objective function is a quadratic form with a symmetric and positive definite matrix, then Newton's method reaches the optimizer from any starting point in just one iteration. 
\item \textbf{Relative weaknesses:} Newton's method may not converge when the starting point is far from the minimizer, the Hessian is not positive definite, or the inverse of the Hessian is expensive, unstable, or otherwise difficult to compute. Newton's method is also very computationally intensive when the dimension of the probblem is large. 
\end{itemize}

\subsubsection*{Conjugate Gradient}
\begin{itemize}
\item \textbf{Basic idea:} The conjugate gradient method moves toward the minimizer of a function by moving along $Q$-conjugate directions. 
\item \textbf{Optimization problems it can and cannot solve:} Conjugate gradient works well for large quadratic minimization problems with sparse matrices. 
\item \textbf{Relative strengths:} Each step of conjugate gradient is generally less expensive to compute than a step in Newton's method. This method never computes or stores the Hessian or approximations to the Hessian. This method is useful for solving large quadratic optimization problems where the matrix is symmetric, positive definite, and sparse. With sparsity, many iterations of conjugate gradient can be less expensive than a single iteration of Newton's method.
\item \textbf{Relative weaknesses:} If the matrices in the objective function are not sparse, then each iteration of conjugate gradient can be very expensive. 
\end{itemize}

\subsection*{Exercise 4}
\begin{claim}
Let $f(x) = \frac{1}{2}x^TQx - b^Tx$, where $Q \in M_n( \mathbb{R})$ satisfies $Q > 0 $ and $b \in \mathbb{R}^n$. The Method of Steepest Descent converges in one step (i.e. $x_1 = Q^{-1}b$) if and only if $x_0$ is chosen such that $Df(x_0)^T = Qx_0 - b$ is an eigenvector of $Q$ (and $\alpha$ is chosen optimally). 
\end{claim}
\begin{proof}
First suppose that the Method of Steepest Descent converges in one step. Then, 
\begin{equation}
x_1 = Q^{-1}b = x_0 - \alpha_0 (Qx_0 - b)
\end{equation}
for some $\alpha_0$. Multiply both sides of the equation by $Q$. Then, 
\begin{equation}
b = Qx_0 - \alpha_0 Q(Qx_0 - b)
\end{equation}
or 
\begin{equation}
Q(Qx_0 - b) = \frac{1}{\alpha_0} (Qx_0 - b)
\end{equation}
Therefore, $(Qx_0 - b)$ is an eigenvector of $Q$. Clearly, since the method converged in one step, $\alpha$ was chosen optimally and must satisfy (9.2). Conversely, suppose that  $Df(x_0)^T = Qx_0 - b$ is an eigenvector of $Q$ with eigenvalue $\lambda$ and $\alpha$ is chosen optimally according to $9.2$. The Method of Steepest Descent implies that,
\begin{align*}
x_1 &= x_0 - \alpha_0 Df(x_0)^T \\
&= x_0 - \frac{Df(x_0)Df(x_0)^T}{Df(x_0)QDf(x_0)^T} Df(x_0)^T \\
&= x_0 - \frac{Df(x_0)Df(x_0)^T}{ \lambda Df(x_0)Df(x_0)^T} Df(x_0)^T\\
&= x_0 - \frac{1}{\lambda} Df(x_0)^T \\
&= x_0 - \frac{1}{\lambda} (Qx_0 - b)
\end{align*}
Now, multiply both sides by $Q$:
\begin{align*}
Qx_1 &= Qx_0 - \frac{1}{\lambda} Q(Qx_0 - b) \\
&= Qx_0 - \frac{1}{\lambda} \lambda (Qx_0 - b) \\
&= Qx_0 - (Qx_0 - b) \\
&= b
\end{align*}
Thus, $x_1 = Q^{-1}b$ so that the Method of Steepest Descent converges in one step.
\end{proof}

\subsection*{Exercise 5}
\begin{claim}
Let $\{x_k\}_{k=0}^{\infty}$ be defined by the Method of Steepest Descent. Then, $x_{k+1} - x_k$ is orthogonal to $x_{k+2} - x_{k+1}$ for each $k$. 
\end{claim}
\begin{proof}
Recall that in the Method of Steepest Descent, $\alpha_k$ is chosen to minimize $f(x_k - \alpha_k Df(x_k)^T)$. Therefore, by the FONC at the optimal $\alpha_k$, we have that,
\begin{equation}
Df(x_k - \alpha_k Df(x_k)^T) Df(x_k)^T = 0
\end{equation}
Also recall that $x_{k+1} - x_k = -\alpha_k Df(x_k)^T$ and $x_{k+2} - x_{k+1} =  -\alpha_{k+1} Df(x_{k+1})^T$. Thus,
\begin{align*}
\inp{x_{k+1} - x_k}{x_{k+2} - x_{k+1}} &= \alpha_k \alpha_{k+1} Df(x_k) Df(x_{k+1})^T \\
&= \alpha_k \alpha_{k+1} Df(x_k - \alpha_k Df(x_k)^T) Df(x_k)^T \\
&= 0 
\end{align*}
Therefore, $x_{k+1} - x_k$ is orthogonal to $x_{k+2} - x_{k+1}$ for each $k$. 
\end{proof}

\subsection*{Exercises 6 - 9: See Jupyter Notebook}

\subsection*{Exercise 10}
\begin{claim}
Let $f(x) = \frac{1}{2} x^T Q x - b^T x$ where $Q \in M_n (\mathbb{R})$ is symmetric and positive definite. For any initial guess $x_0 \in R^n$, one iteration of Newton's method lands at the unique minimizer of $f$. 
\end{claim}
\begin{proof}
First observe that $Df(x_0)^T = Qx_0 - b$ and $D^2f(x_0) = Q$. Therefore, 
\begin{align*}
x_1 &= x_0 - D^2f(x_0)^{-1} Df(x_0)^T \\
&= x_0 - Q^{-1} (Qx_0 - b) \\
&= x_0 - x_0 +  Q^{-1}b \\
&= Q^{-1}b
\end{align*}
Then, since $Q$ is positive definite, this system of equations will have a unique solution so that $x_1 = Q^{-1}b$ is the unique minimizer of $f$ by the FONC and second-order sufficient condition. 
\end{proof}
\subsection*{Exercise 12}
\begin{claim}
	If $A \in M_n (\mathbb{F})$ has eigenvalues $\lambda_1, \ldots, \lambda_n$ and $B = A + \mu I$, then the eigenvectors of $A$ and $B$ are the same, and the eigenvalues of $B$ are $\mu + \lambda_1, \mu + \lambda_2, \ldots, \mu + \lambda_n$. 
\end{claim}
\begin{proof}
	Let $\lambda_i$ and $x_i$ be an eigenvalue and eigenvector, respectively, of $A$. Then,
	\begin{align*}
		Bx_i &= (A + \mu I)x_i \\
		&= Ax_i + \mu I x_i \\
		&= \lambda_i  x_i+ \mu x_i \\
		&= (\lambda_i + \mu) x_i
	\end{align*}
	Therefore, $(\lambda_i + \mu)$ is an eigenvalue of $B$ with eigenvector $x_i$.  It follows that $A$ and $B$ have the same eigenvectors, and the eigenvalues of $B$ are $\mu + \lambda_1, \mu + \lambda_2, \ldots, \mu + \lambda_n$. 
\end{proof}

\subsection*{Exercise 15}
\begin{claim}[Sherman-Morrison-Woodbury]
	Let $A$ be a nonsingular $n \times n$ matrix, $B$ an $n \times l$ matrix, $C$ a nonsingular $l \times l$ matrix, and $D$ an $l \times n$ matrix. We have,
	\begin{equation}
	(A + BCD)^{-1} = A^{-1} - A^{-1} B (C^{-1} + DA^{-1}B)^{-1} D A^{-1}
	\end{equation}
\end{claim}
\begin{proof}
	To prove this identity, we show that $(A + BCD) (A^{-1} - A^{-1} B (C^{-1} + DA^{-1}B)^{-1} D A^{-1}) = I$. To that end,
	\begin{align*}
	&(A + BCD) (A^{-1} - A^{-1} B (C^{-1} + DA^{-1}B)^{-1} D A^{-1}) \\
	&= AA^{-1} - AA^{-1} B (C^{-1} + DA^{-1}B)^{-1} D A^{-1} \\
	&\qquad + BCDA^{-1} - BCDA^{-1} B (C^{-1} + DA^{-1}B)^{-1} D A^{-1} \\
	&= I + BCDA^{-1} \\
	&\qquad - (B (C^{-1} + DA^{-1}B)^{-1} + BCDA^{-1} B (C^{-1} + DA^{-1}B)^{-1}) D A^{-1} \\
	&= I + BCDA^{-1} \\
	&\qquad - (BCC^{-1} + BCDA^{-1}B)(C^{-1} + DA^{-1}B)^{-1}) D A^{-1} \\
	&= I + BCDA^{-1} \\
	&\qquad - BC(C^{-1} + DA^{-1}B)(C^{-1} + DA^{-1}B)^{-1}) D A^{-1} \\
	&= I + BCDA^{-1} - BCDA^{-1} \\
	&= I
	\end{align*}
	Therefore, $(A + BCD)^{-1} = A^{-1} - A^{-1} B (C^{-1} + DA^{-1}B)^{-1} D A^{-1}$.
\end{proof}

\subsection*{Exercise 16}
We have that 
\begin{equation}
A_{k+1} = A_k + \frac{y_k - A_k s_k}{\norm{s_k}^2} s^T_k
\end{equation}
We can use the Sherman-Morrison-Woodbury Identity to invert $A_{k+1}$. First observe that $A_k$ is $n\times n$, $y_k - A_k s_k$ is $n \times 1$, $\norm{s_k}^2$ is $1\times 1$, and $s^T_k$ is $1 \times n$. These dimensions suggest the following correspondence for applying the identity: $A = A_k$, $B = y_k - A_k s_k$, $C = \frac{1}{\norm{s_k}^2}$, and $D = s^T_k$. 
Then,
\begin{align*}
	A_{k+1}^{-1} &= (A + BCD)^{-1} \\
	&= A^{-1} - A^{-1} B (C^{-1} + DA^{-1}B)^{-1} D A^{-1} \\
	&= A_k^{-1} - A_k^{-1} \frac{(y_k - A_k s_k)s^T_k A^{-1}_k}{(\norm{s_k}^2 + s^T_k A^{-1}_k (y_k - A_k s_k))} \\
	&= A_k^{-1} + \frac{(s_k - A^{-1}_k y_k)s^T_kA^{-1}_k}{s^T_k A^{-1}_k y_k }
\end{align*}	

\subsection*{Exercise 18}
\begin{claim}
Let $Q \in M_n(\mathbb{R})$ satisfy $Q > 0$, and let $f$ be the quadratic function $f(x) = \frac{1}{2}x^TQx - b^Tx + c$. Given a starting point $x_0$ and $Q$-conjugate directions $d_0, d_1, \ldots, d_{n-1}$ in $\mathbb{R}^n$, the optimal line search solution for $x_{k+1} = x_k + \alpha_k d_k$ is given by $\alpha_k = \frac{r^T_k d_k}{d^T_k Q d_k}$, where $r_k = b- Qx_k$. 
\end{claim}
\begin{proof}
	The optimal line search solution for $x_{k+1} = x_k + \alpha_k d_k$ must satisfy $0 = f'(x_k + \alpha_k d_k)$. Then,
	\begin{align*}
	0 &= f'(x_k + \alpha_k d_k) \\
	&= Df(x_k + \alpha_k d_k) d_k \\
	&= ((x_k + \alpha_k d_k)^T Q - b^T)d_k \\
	&= x_k^T Q d_k + \alpha_k  d_k^T Q d_k - b^T d_k 
	\end{align*}
	Solving this equation for $\alpha_k$ yields, $\alpha_k = \frac{r^T_k d_k}{d^T_k Q d_k}$, where $r_k = b- Qx_k$. 
\end{proof}

\subsection*{Exercise 20}
\begin{claim}
In the Conjugate Gradient Algorithm, $r^T_i r_k = 0$ for all $i <k $. 
\end{claim}
\begin{proof}
I prove this statement by induction. Recall that $r_0 = d_0$. Then,
\begin{align*}
	r_0^T r_1 &= r_0^T(b - Qx_1) \\
	&= r_0^T(b - Qx_0 - a_0 Q d_0) \\
	&= r_0^T r_0 - a_0 r_0^T A r_0 \\
	&= r_0^T r_0 - \frac{r_0^T r_0}{r_0^T Q r_0} r_0^T A r_0 \\ 
	&=  r_0^T r_0 -  r_0^T r_0 = 0
\end{align*}
Therefore, $r_0^T r_1$. This pattern will continue for all $i <k $. Thus, by induction, we have that $r^T_i r_k = 0$ for all $i <k $. 
\end{proof}

\end{document}